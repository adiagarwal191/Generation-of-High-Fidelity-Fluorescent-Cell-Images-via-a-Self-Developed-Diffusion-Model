{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adiagarwal191/Generation-of-High-Fidelity-Fluorescent-Cell-Images-via-a-Self-Developed-Diffusion-Model/blob/main/FYP_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn1w2WgdWuC4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional\n",
        "from diffusers import UNet2DModel\n",
        "from torch.optim import Adam\n",
        "from torch.amp import autocast, GradScaler\n",
        "import torch.amp\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-KKHKOyhmVXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlX9S5jGXIMb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajeGpPXLXS-Y"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "  image_size = (256,256)\n",
        "  num_epochs = 10\n",
        "  learning_rate = 0.0001\n",
        "  output_dir = '/content/drive/MyDrive/gen_images' #This path can be changed based on folder and path names\n",
        "  T = 100\n",
        "  batch_size = 8\n",
        "config = config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q0cSaxOfaUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9dc9dbd-991e-4200-b884-df780d7947ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total images = 1440\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/all_images'\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #random crop here\n",
        "    transforms.RandomCrop(256), #Randomly crops images of size 256x256 from any location in the image\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "class imageDataSet(Dataset):\n",
        "  def __init__(self,folder_path, transform):\n",
        "    self.paths = glob.glob(os.path.join(folder_path, '*'))\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_path = self.paths[idx]\n",
        "    img = Image.open(img_path)\n",
        "    # print(img_path)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img\n",
        "\n",
        "dataset = imageDataSet(folder_path = data_path, transform = transform)\n",
        "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "print(f\"total images = {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNMUE1zsut8W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvhKu1OpXu4k",
        "outputId": "bb7eb458-40c1-45aa-d97f-37a65d531184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n"
          ]
        }
      ],
      "source": [
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(64,64,128, 128, 256, 256),  # Consider reducing these numbers if needed.\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "if hasattr(model, \"enable_gradient_checkpointing\"):\n",
        "    model.enable_gradient_checkpointing()\n",
        "print(\"Model loaded on\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q87vUEHvKvGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2fea9e-9336-4d5d-9fe9-2cbb326b48e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "    print(batch.shape)  #Cell to check if model has processed the dataset correctly\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bmlU6UjX6Of"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps , s=0.008): #Function that defines the noise schedule\n",
        "  t = torch.linspace(0 , timesteps, timesteps + 1)\n",
        "  alphas_cumprod = torch.cos(((t / timesteps + s) / (1 + s)) * (torch.pi / 2)) ** 2\n",
        "  alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "  betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
        "  betas = torch.clamp(betas, 0, 0.999)\n",
        "  return betas\n",
        "\n",
        "#Code below is to calculate the posterior variance\n",
        "betas = cosine_beta_schedule(config.T).to(device)\n",
        "alphas = 1 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device = device), alphas_cumprod[:-1]], dim=0)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpIhHNqyX9KW"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion_sample(x_0, t): #Applies noise to the image and returns the noisy image and the amount of noise added\n",
        "  t = t.long()\n",
        "  alphas_cumprod_t = alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "  sqrt_alpha_cumprod_t = torch.sqrt(alphas_cumprod_t)\n",
        "  sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alphas_cumprod_t)\n",
        "  noise = torch.randn_like(x_0)\n",
        "  x_t = sqrt_alpha_cumprod_t * x_0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
        "  return x_t, noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLzGLZiseny-"
      },
      "outputs": [],
      "source": [
        "def ddpm_loss(model, x_0, t): #Function to calculate the loss between the noise added and the model's prediction\n",
        "  x_0 = x_0.to(device)\n",
        "  t = t.to(device)\n",
        "  x_t, noise = forward_diffusion_sample(x_0, t)\n",
        "  noise_pred = model(x_t, t).sample\n",
        "  lossMSE = nn.MSELoss()\n",
        "  output = lossMSE(noise_pred, noise)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyAptDxpfIPq"
      },
      "outputs": [],
      "source": [
        "# model_load = torch.load('/content/drive/MyDrive/gen_images/diffusion_model.pth', map_location=torch.device('cpu'))\n",
        "# model.load_state_dict(model_load)\n",
        "# The above code is if you wish to load the model from a saved state\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, epochs, learning_rate): #The training loop for the model\n",
        "  optimiser = Adam(model.parameters(), lr = config.learning_rate)\n",
        "  scaler = GradScaler()\n",
        "  model.train()\n",
        "  z = 0\n",
        "  for epoch in range(epochs):\n",
        "    for step, batch_images in enumerate(dataloader):\n",
        "\n",
        "      t = torch.randint(0, config.T, (batch_images.size(0),), device=device)\n",
        "\n",
        "      optimiser.zero_grad()\n",
        "      with autocast(device_type=\"cuda\"):\n",
        "          loss = ddpm_loss(model, batch_images, t)\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      scaler.step(optimiser)\n",
        "      scaler.update()\n",
        "\n",
        "      if (step% 5 == 0):\n",
        "        print (f\"Epoch: {epoch} | step: {step} | loss: {loss.item()}\")\n",
        "\n",
        "      if (step % 20 == 0):\n",
        "        z = z + 1\n",
        "        print(\"Sampling images from the trained model...\")\n",
        "        generated = sample(model, sample_size=1)  #The number of samples can be changed, however this will affect training times\n",
        "        for i in range(generated.size(0)):\n",
        "          img_np = generated[i].permute(1,2,0).cpu().numpy()\n",
        "          plt.figure(figsize=(6,6))\n",
        "          plt.imshow(img_np)\n",
        "          plt.axis('off')\n",
        "          plt.title(f\"Generated sample {z}\")\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "    save_path = os.path.join(config.output_dir, f\"epoch_{epoch}_image.png\")\n",
        "\n",
        "\n",
        "  model_save_path = os.path.join(config.output_dir, \"diffusion_model.pth\")\n",
        "  torch.save(model.state_dict(), model_save_path) #Saves the trained model\n",
        "  print(f\"Model saved to {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwg9KRkBc3Dr"
      },
      "outputs": [],
      "source": [
        "def sample(model, sample_size = 1): #Code for the reverse diffusion process\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      # Start from pure noise\n",
        "    x = torch.randn(sample_size, 3, config.image_size[1], config.image_size[0], device=device)\n",
        "\n",
        "    for i in reversed(range(config.T)):\n",
        "        # print(i)\n",
        "        t = torch.tensor([i]*sample_size, device=device, dtype=torch.long)\n",
        "        beta_t = betas[t].view(-1,1,1,1).to(device)\n",
        "        alpha_t = alphas[t].view(-1,1,1,1).to(device)\n",
        "        alpha_cumprod_t = alphas_cumprod[t].view(-1,1,1,1).to(device)\n",
        "        alpha_cumprod_t_prev = alphas_cumprod_prev[t].view(-1,1,1,1).to(device)\n",
        "\n",
        "        model_pred = model(x, t).sample\n",
        "        one_over_sqrt_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
        "        coeff_model_pred = (beta_t / torch.sqrt(1 - alpha_cumprod_t))\n",
        "        x_0_pred_part = x - coeff_model_pred * model_pred\n",
        "        x_0_pred_part = one_over_sqrt_alpha_t * x_0_pred_part\n",
        "\n",
        "        if i > 0:\n",
        "            sigma_t = torch.sqrt((1 - alpha_cumprod_t_prev)/(1 - alpha_cumprod_t)*beta_t)\n",
        "            z = torch.randn_like(x)\n",
        "            x = x_0_pred_part + (sigma_t * z)\n",
        "        else:\n",
        "            x = x_0_pred_part\n",
        "\n",
        "    x = x.clamp(0,1)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atCdFPTdhb7y"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\": #This begins the training\n",
        "  print(\"Starting training...\")\n",
        "  train_model(model, dataloader, epochs=config.num_epochs, learning_rate=config.learning_rate)\n",
        "\n",
        "  print(\"Sampling images from the trained model...\")\n",
        "  generated = sample(model, sample_size=2)\n",
        "\n",
        "  for i in range(generated.size(0)):\n",
        "      img_np = generated[i].permute(1,2,0).cpu().numpy()\n",
        "      plt.figure(figsize=(6,6))\n",
        "      plt.imshow(img_np)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"Generated sample {i}\")\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmZPFP2RueiH"
      },
      "outputs": [],
      "source": [
        "  #This code snippet is to test the images generated after the model has been trained\n",
        "  print(\"Sampling images from the trained model...\")\n",
        "  generated = sample(model, sample_size=16)\n",
        "\n",
        "\n",
        "  for i in range(generated.size(0)):\n",
        "      img_np = generated[i].permute(1,2,0).cpu().numpy()\n",
        "      plt.figure(figsize=(6,6))\n",
        "      plt.imshow(img_np)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"Generated sample {i}\")\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Zp6w7f8ibp1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1c1JXlY9d7Z4aFEcXixfeoBg5_S-LLePH",
      "authorship_tag": "ABX9TyPHKgxECo3f8/8uQTQ9qH6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}